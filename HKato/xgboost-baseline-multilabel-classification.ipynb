{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from category_encoders import CountEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framing as a binary classification problem\n",
    "\n",
    "In this notebook I create a baseline model using XGBoost and framing the problem as a n-binary classification problems (where n=206 and is the total number of classes). I make use of the [MultiOutputClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html#sklearn.multioutput.MultiOutputClassifier) wrapper in sklearn.\n",
    "\n",
    "This has the advantages that :\n",
    "- You can use models capable only of binary classification\n",
    "- It is easy to implement\n",
    "\n",
    "But has the disadvantages that:\n",
    "- You lose any correlation between labels which could be useful to the model\n",
    "- You need to train *n* models and is therefore slow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "NFOLDS = 5\n",
    "\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/lish-moa/train_features.csv')\n",
    "targets = pd.read_csv('/kaggle/input/lish-moa/train_targets_scored.csv')\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lish-moa/test_features.csv')\n",
    "sub = pd.read_csv('/kaggle/input/lish-moa/sample_submission.csv')\n",
    "\n",
    "# drop id col\n",
    "X = train.iloc[:,1:].to_numpy()\n",
    "X_test = test.iloc[:,1:].to_numpy()\n",
    "y = targets.iloc[:,1:].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n",
    "                ('classify', MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist')))\n",
    "               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'classify__estimator__colsample_bytree': 0.652231655518253,\n",
    "          'classify__estimator__gamma': 3.6975211709521023,\n",
    "          'classify__estimator__learning_rate': 0.05033414197773552,\n",
    "          'classify__estimator__max_delta_step': 2.070593162427692,\n",
    "          'classify__estimator__max_depth': 10,\n",
    "          'classify__estimator__min_child_weight': 31.579959348704868,\n",
    "          'classify__estimator__n_estimators': 166,\n",
    "          'classify__estimator__subsample': 0.8638628715886625,\n",
    "          'encode__min_group_size': 0.4160029192647806}\n",
    "\n",
    "clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train the model\n",
    "\n",
    "Framing this problem as a binary classification problem has the disadvantage that you need to train as many models as you have classes. For this problem this means training 206 models per fold, for the large number of features included in this dataset this may take a long time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = np.zeros(y.shape)\n",
    "test_preds = np.zeros((test.shape[0], y.shape[1]))\n",
    "kf = KFold(n_splits=NFOLDS)\n",
    "for fn, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print('Starting fold: ', fn)\n",
    "    X_train, X_val = X[trn_idx], X[val_idx]\n",
    "    y_train, y_val = y[trn_idx], y[val_idx]\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_preds = clf.predict_proba(X_val) # list of preds per class\n",
    "    val_preds = np.array(val_preds)[:,:,1].T # take the positive class\n",
    "    oof_preds[val_idx] = val_preds\n",
    "    \n",
    "    preds = clf.predict_proba(X_test)\n",
    "    preds = np.array(preds)[:,:,1].T # take the positive class\n",
    "    test_preds += preds / NFOLDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('OOF log loss: ', log_loss(np.ravel(y), np.ravel(oof_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of OOF preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file\n",
    "sub.iloc[:,1:] = test_preds\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
